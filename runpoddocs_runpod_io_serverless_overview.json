{
    "content": "Overview\nRunPod offers Serverless GPU computing for AI inference, training, and general compute, allowing\nusers to pay by the second for their compute usage. This \ufb02exible platform is designed to scale\ndynamically, meeting the computational needs of AI workloads from the smallest to the largest\nscales.\nYou can use the following methods:\nQuick Deploy: Quick deploys are pre-built custom endpoints of the most popular AI models.\nHandler Functions: Bring your own functions and run in the cloud.\nIf you'd like to use pre-built endpoints of some of the most popular AI models, see AI APIs.\nWhy RunPod Serverless?\nYou should choose RunPod Serverless instances for the following reasons:\nAI Inference: Handle millions of inference requests daily and can be scaled to handle billions,\nmaking it an ideal solution for machine learning inference tasks. This allows users to scale their\nmachine learning inference while keeping costs low.\nAI Training: Machine learning training tasks that can take up to 12 hours. GPUs can be spun up\nper request and scaled down once the task is done, providing a \ufb02exible solution for AI training\nneeds.\nAutoscale: Dynamically scale workers from 0 to 100 on the Secure Cloud platform, which is\nhighly available and distributed globally. This provides users with the computational resources\nexactly when needed.\nContainer Support: Bring any Docker container to RunPod. Both public and private image\nrepositories are supported, allowing users to con\ufb01gure their environment exactly how they\nwant.\n3s Cold-Start: To help reduce cold-start times, RunPod proactively pre-warms workers. The\ntotal start time will vary based on the runtime, but for stable diffusion, the total start time is 3\nseconds cold-start plus 5 seconds runtime.\nConvert web pages and HTML files to PDF in your applications with the Pdfcrowd HTML to PDF API\nPrinted with Pdfcrowd.com\nMetrics and Debugging: Transparency is vital in debugging. RunPod provides access to GPU,\nCPU, Memory, and other metrics to help users understand their computational workloads. Full\ndebugging capabilities for workers through logs and SSH are also available, with a web terminal\nfor even easier access.\nWebhooks: Users can leverage webhooks to get data output as soon as a request is done. Data\nis pushed directly to the user's Webhook API, providing instant access to results.\nRunPod Serverless GPUs are not just for AI Inference and Training. They're also great for a variety of\nother use cases. Feel free to use them for tasks like rendering, molecular dynamics, or any other\ncomputational task that suits your fancy.\nHow to interact with RunPod Serverless?\nRunPod generates an Endpoint Id that that allows you to interact with your Serverless Pod. Pass in\nyour Endpoint Id to the Endpoint URL and provide an operation.\nThis Endpoint URL will look like this:\napi.runpod.ai : The base URL to access RunPod.\nv2 : The API version.\nendpoint_id : The ID of the Serverless Endpoint.\noperation : The operation to perform on the Serverless Endpoint.\nValid options: run  | runsync  | status  | cancel  | health  | purge\nEdit this page\nhttps://api.runpod.ai/v2/{endpoint_id}/{operation}\nConvert web pages and HTML files to PDF in your applications with the Pdfcrowd HTML to PDF API\nPrinted with Pdfcrowd.com\n"
}