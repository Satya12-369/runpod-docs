{
    "content": "Glossary\nOn this page\nGlossary\nCommunity Cloud\nGPU instances connect individual compute providers to consumers through a vetted, secure\npeer-to-peer system.\nDatacenter\nA data center is a secure location where RunPod's cloud computing services, such as Secure\nCloud and GPU Instances, are hosted. These data centers are equipped with redundancy and\ndata backups to ensure the safety and reliability of your data.\nEndpoint\nAn Endpoint refers to a speci\ufb01c URL where your serverless applications or services can be\naccessed. These endpoints provide standard functionality for submitting jobs and retrieving the\noutput from job requests.\nGPU Instance\nGPU Instance is a container-based GPU instance that you can deploy. These instances spin up in\nseconds using both public and private repositories. They are available in two different types:\nSecure Cloud and Community Cloud.\nHandler\nA Handler is a function that is responsible for processing submitted inputs and generating the\nresulting output.\nAsk AI\nAsk AI\nRunPod\nRunPod\nRunPod is a cloud computing platform primarily designed for AI and machine learning\napplications.\nSecure Cloud\nGPU instances that run in T3/T4 data centers, providing high reliability and security.\nServerless GPU\nServerless GPU is a pay-per-second serverless GPU computing solution. It is designed to bring\nautoscaling to your production environment, meaning it can dynamically adjust computational\nresources based on your application's needs.\nTemplate\nA RunPod template is a Docker container image paired with a con\ufb01guration.\nEdit this page\nPrevious\n\u00ab Video resources\nDocs\nOverview\nTutorials\nAI APIs\nCommunity\nDiscord\nAsk AI\nAsk AI\nContact us\nMore\nBlog\nGitHub\nCopyright \u00a9 2024 RunPod\nAsk AI\nAsk AI\n"
}