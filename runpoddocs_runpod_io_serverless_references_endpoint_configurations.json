{
    "content": "Serverless\nReferences\nEndpoint con\ufb01gurations\nOn this page\nEndpoint con\ufb01gurations\nThe following are con\ufb01gurable settings within an Endpoint.\nEndpoint Name\nCreate a name you'd like to use for the Endpoint con\ufb01guration. The resulting endpoint will be\nassigned a random ID to be used when making calls.\nThe name is only visible to you.\nGPU Selection\nSelect one or more GPUs you want your Endpoint to run on.\nActive (Min) Workers\nSetting this amount to 1 will result in \"always on\" workers. This will allow you to have a worker\nready to respond to job requests without incurring any cold start delay.\nNOTE\nYou will incur the cost of any active workers you have set regardless if they are working on\na job.\nMax Workers\nThis will establish a ceiling or upper limit to the number of active workers your endpoint will have\nrunning at any given point.\nDefault: 3\nAsk AI\nAsk AI\nRunPod\nGPUs / Worker\nThe number of GPUs you would like assigned to your worker.\nNOTE\nCurrently only available for 48GB GPUs.\nIdle Timeout\nThe amount of time in seconds a worker not currently processing a job will remain active until it\nis put back into standby. During the idle period, your worker is considered running and will incur\na charge.\nDefault: 5 seconds\nFlashBoot\nRunPod magic to further reduce the average cold-start time of your endpoint. FlashBoot works\nbest when an endpoint receives consistent utilization. There is no additional cost associated\nwith FlashBoot.\nAdvanced\nAdditional controls to help you control where your endpoint is deployed and how it responds to\nincoming requests.\nData Centers\nControl which datacenters you would like your workers deployed and cached. By default all\ndatacenters are selected.\nSelect Network Volume\nAttach a network storage volume to your deployed workers.\nHow to con\ufb01gure Max Workers\nAsk AI\nAsk AI\nNetwork volumes will be mounted to /runpod-volume/ .\nNOTE\nWhile this is a high performance network drive, do keep in mind that it will have higher\nlatency than a local drive.\nThis will limit the availability of cards, as your endpoint workers will be locked to the\ndatacenter that houses your network volume.\nScale Type\nQueue Delay scaling strategy adjusts worker numbers based on request wait times. With\nzero workers initially, the \ufb01rst request adds one worker. Subsequent requests add workers\nonly after waiting in the queue for the de\ufb01ned number of delay seconds.\nRequest Count scaling strategy adjusts worker numbers according to total requests in the\nqueue and in progress. It automatically adds workers as the number of requests increases,\nensuring tasks are handled e\ufb03ciently.\nGPU Types\nWithin the select GPU size category you can further select which GPU models you would like\nyour endpoint workers to run on. Default: 4090  | A4000  | A4500\nEdit this page\nPrevious\n\u00ab References\nNext\nEndpoint operations \u00bb\n_Total Workers Formula: Math.ceil((requestsInQueue + requestsInProgress) /\nWhat's the difference between GPU models.\nAsk AI\nAsk AI\nDocs\nOverview\nTutorials\nAI APIs\nCommunity\nDiscord\nContact us\nMore\nBlog\nGitHub\nCopyright \u00a9 2024 RunPod\nAsk AI\nAsk AI\n"
}