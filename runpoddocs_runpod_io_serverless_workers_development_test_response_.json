{
    "content": "Serverless\nWorkers\nDevelopment\nTest response time\nTest response time\nWhen setting up an API, you have several options available at different price points and resource\nallocations. You can select a single option if you would prefer to only use one price point, or\nselect a preference order between the pools that will allocate your requests accordingly.\nThe option that will be most cost effective for you will be based on your use case and your\ntolerance for task run time. Each situation will be different, so when deciding which API to use,\nit's worth it to do some testing to not only \ufb01nd out how long your tasks will take to run, but how\nmuch you might expect to pay for each task.\nTo \ufb01nd out how long a task will take to run, select a single pool type as shown in the image\nabove. Then, you can send a request to the API through your preferred method. If you're\nunfamiliar with how to do so or don't have your own method, then you can use a free option like\nreqbin.com to send an API request to the RunPod severs.\nThe URLs to use in the API will be shown in the My APIs screen:\nAsk AI\nAsk AI\nRunPod\nOn reqbin.com, enter the Run URL of your API, select POST under the dropdown, and enter your\nAPI key that was given when you created the key under Settings(if you do not have it saved, you\nwill need to return to Settings and create a new key). Under Content, you will also need to give it\na basic command (in this example, we've used a Stable Diffusion prompt).\nAsk AI\nAsk AI\nSend the request, and it will give you an ID for the request and notify you that it is processing.\nYou can then swap the URL in the request \ufb01eld with the Status address and add the ID to the end\nof it, and click Send.\nIt will return a Delay Time and an Execution Time, denoted in milliseconds. The Delay Time\nshould be extremely minimal, unless the API process was spun up from a cold start, then a\nsizable delay is expected for the \ufb01rst request sent. The Execution Time is how long the GPU took\nto actually process the request once it was received. It may be a good idea to send a number of\ntests so you can get a min, max, and average run time -- \ufb01ve tests should be an adequate sample\nsize.\nAsk AI\nAsk AI\nYou can then switch the GPU pool above to a different pool and repeat the process.\nWhat will ultimately be right for your use case will be determined by how long you can afford to\nlet the process run. For heavier jobs, a task on a slower GPU will be likely be more cost-effective\nwith a tradeoff of speed. For simpler tasks, there may also be diminishing returns on how fast\nthe task that can be run that may not be signi\ufb01cantly improved by selecting higher-end GPUs.\nExperiment to \ufb01nd the best balance for your scenario.\nEdit this page\nPrevious\n\u00ab Use environment variables\nNext\nDeploy \u00bb\nDocs\nOverview\nTutorials\nAI APIs\nAsk AI\nAsk AI\nCommunity\nDiscord\nContact us\nMore\nBlog\nGitHub\nCopyright \u00a9 2024 RunPod\nAsk AI\nAsk AI\n"
}